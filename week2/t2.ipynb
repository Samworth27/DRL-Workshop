{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym==0.25.1 in /home/sam/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/lib/python3/dist-packages (from gym==0.25.1) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/sam/.local/lib/python3.10/site-packages (from gym==0.25.1) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sam/.local/lib/python3.10/site-packages (from gym==0.25.1) (2.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym[classic_control]==0.25.1 in /home/sam/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/lib/python3/dist-packages (from gym[classic_control]==0.25.1) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/sam/.local/lib/python3.10/site-packages (from gym[classic_control]==0.25.1) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sam/.local/lib/python3.10/site-packages (from gym[classic_control]==0.25.1) (2.1.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/sam/.local/lib/python3.10/site-packages (from gym[classic_control]==0.25.1) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install gym==0.25.1\n",
    "!{sys.executable} -m pip install gym[classic_control]==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from IPython import display\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/sam/.local/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95270/1817636120.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n",
      "/home/sam/.local/lib/python3.10/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.0007596485614776612\n",
      "Mean Reward: 0.015\n",
      "Time Average: 0.0006046288013458252\n",
      "Mean Reward: 22.068\n",
      "Episode: 2000\n",
      "Time Average: 0.00127907395362854\n",
      "Mean Reward: 22.504\n",
      "Time Average: 0.0006143548488616944\n",
      "Mean Reward: 22.314\n",
      "Episode: 4000\n",
      "Time Average: 0.0007833573818206787\n",
      "Mean Reward: 21.801\n",
      "Time Average: 0.0005968797206878662\n",
      "Mean Reward: 21.81\n",
      "Episode: 6000\n",
      "Time Average: 0.0010742180347442626\n",
      "Mean Reward: 22.745\n",
      "Time Average: 0.0006068096160888671\n",
      "Mean Reward: 22.201\n",
      "Episode: 8000\n",
      "Time Average: 0.0010612168312072753\n",
      "Mean Reward: 22.008\n",
      "Time Average: 0.0006324729919433594\n",
      "Mean Reward: 22.53\n",
      "Episode: 10000\n",
      "Time Average: 0.0009132781028747558\n",
      "Mean Reward: 21.342\n",
      "Epsilon: 0.9753093024395111\n",
      "Epsilon: 0.9512282354250458\n",
      "Time Average: 0.0006210019588470459\n",
      "Mean Reward: 22.511\n",
      "Epsilon: 0.9277417467531685\n",
      "Episode: 12000\n",
      "Time Average: 0.0008816008567810059\n",
      "Mean Reward: 24.537\n",
      "Epsilon: 0.8607047486686201\n",
      "Time Average: 0.0007172863483428955\n",
      "Mean Reward: 25.728\n",
      "Episode: 14000\n",
      "Time Average: 0.0010868840217590331\n",
      "Mean Reward: 26.949\n",
      "Epsilon: 0.7985117269685725\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.0008110229969024659\n",
      "Mean Reward: 28.784\n",
      "Episode: 16000\n",
      "Time Average: 0.0010762679576873779\n",
      "Mean Reward: 31.523\n",
      "Time Average: 0.0009199073314666748\n",
      "Mean Reward: 32.656\n",
      "Episode: 18000\n",
      "Time Average: 0.001399848222732544\n",
      "Mean Reward: 36.328\n",
      "Epsilon: 0.6537628386312633\n",
      "Epsilon: 0.6376209781063321\n",
      "Time Average: 0.0011538217067718507\n",
      "Mean Reward: 39.863\n",
      "Epsilon: 0.6218776713776856\n",
      "Episode: 20000\n",
      "Epsilon: 0.606523077874078\n",
      "Time Average: 0.002040771722793579\n",
      "Mean Reward: 42.528\n",
      "Epsilon: 0.5915475999948323\n",
      "Time Average: 0.001354578733444214\n",
      "Mean Reward: 48.156\n",
      "Episode: 22000\n",
      "Epsilon: 0.5488034037068503\n",
      "Time Average: 0.0021196537017822265\n",
      "Mean Reward: 50.119\n",
      "Epsilon: 0.5220372933033263\n",
      "Time Average: 0.0015610826015472412\n",
      "Mean Reward: 56.009\n",
      "Episode: 24000\n",
      "Epsilon: 0.4965766133349901\n",
      "Time Average: 0.0035658092498779296\n",
      "Mean Reward: 58.862\n",
      "Epsilon: 0.484315790359524\n",
      "Time Average: 0.00178843092918396\n",
      "Mean Reward: 64.246\n",
      "Epsilon: 0.4606948546521764\n",
      "Episode: 26000\n",
      "Time Average: 0.0023078956604003905\n",
      "Mean Reward: 70.094\n",
      "Epsilon: 0.43822595366018774\n",
      "Epsilon: 0.4274058491752072\n",
      "Time Average: 0.0020831356048583985\n",
      "Mean Reward: 75.148\n",
      "Epsilon: 0.41685290061763824\n",
      "Episode: 28000\n",
      "Epsilon: 0.4065605117212756\n",
      "Time Average: 0.003266210079193115\n",
      "Mean Reward: 81.962\n",
      "Epsilon: 0.396522249086328\n",
      "Time Average: 0.002253185510635376\n",
      "Mean Reward: 81.863\n",
      "Epsilon: 0.3771831593051582\n",
      "Episode: 30000\n",
      "Time Average: 0.004877540111541748\n",
      "Mean Reward: 90.811\n",
      "Epsilon: 0.3499285630596461\n",
      "Time Average: 0.002676879644393921\n",
      "Mean Reward: 97.175\n",
      "Episode: 32000\n",
      "Time Average: 0.003089482545852661\n",
      "Mean Reward: 97.302\n",
      "Epsilon: 0.32464333633178233\n",
      "Time Average: 0.002758545398712158\n",
      "Mean Reward: 100.357\n",
      "Episode: 34000\n",
      "Epsilon: 0.3011851759202241\n",
      "Time Average: 0.006557035207748413\n",
      "Mean Reward: 116.682\n",
      "Epsilon: 0.29374870383187524\n",
      "Time Average: 0.0031319005489349365\n",
      "Mean Reward: 111.67\n",
      "Epsilon: 0.27942206120438906\n",
      "Episode: 36000\n",
      "Time Average: 0.004446954488754273\n",
      "Mean Reward: 125.53\n",
      "Epsilon: 0.25923151114313064\n",
      "Time Average: 0.003269593000411987\n",
      "Mean Reward: 118.293\n",
      "Episode: 38000\n",
      "Time Average: 0.004646225214004517\n",
      "Mean Reward: 120.307\n",
      "Epsilon: 0.24049989496139146\n",
      "Epsilon: 0.23456178479157042\n",
      "Time Average: 0.003790128231048584\n",
      "Mean Reward: 139.124\n",
      "Episode: 40000\n",
      "Epsilon: 0.22312179264543486\n",
      "Time Average: 0.007074600696563721\n",
      "Mean Reward: 151.524\n",
      "Time Average: 0.004607961654663086\n",
      "Mean Reward: 168.3\n",
      "Epsilon: 0.206999401647574\n",
      "Episode: 42000\n",
      "Time Average: 0.00785559868812561\n",
      "Mean Reward: 149.535\n",
      "Epsilon: 0.19690367556326213\n",
      "Epsilon: 0.192041986461381\n",
      "Time Average: 0.004915566682815552\n",
      "Mean Reward: 177.738\n",
      "Epsilon: 0.18730033585474753\n",
      "Episode: 44000\n",
      "Time Average: 0.007190695762634277\n",
      "Mean Reward: 158.771\n",
      "Epsilon: 0.17376634075333858\n",
      "Time Average: 0.004033472776412964\n",
      "Mean Reward: 148.999\n",
      "Episode: 46000\n",
      "Time Average: 0.008288779973983765\n",
      "Mean Reward: 171.777\n",
      "Epsilon: 0.15722989402047993\n",
      "Time Average: 0.005506011247634887\n",
      "Mean Reward: 201.898\n",
      "Episode: 48000\n",
      "Epsilon: 0.1495615146451681\n",
      "Time Average: 0.013200873136520385\n",
      "Mean Reward: 203.571\n",
      "Time Average: 0.005197383642196655\n",
      "Mean Reward: 192.199\n",
      "Epsilon: 0.13875446084395782\n",
      "Episode: 50000\n",
      "Time Average: 0.006628222703933716\n",
      "Mean Reward: 206.016\n",
      "Time Average: 0.005971008777618408\n",
      "Mean Reward: 220.969\n",
      "Epsilon: 0.12554991420537906\n",
      "Episode: 52000\n",
      "Epsilon: 0.12244999924498873\n",
      "Time Average: 0.010288641691207885\n",
      "Mean Reward: 226.337\n",
      "Epsilon: 0.11942662334734862\n",
      "Time Average: 0.005219774484634399\n",
      "Mean Reward: 193.649\n",
      "Epsilon: 0.11360197618947\n",
      "Episode: 54000\n",
      "Time Average: 0.01005963134765625\n",
      "Mean Reward: 231.769\n",
      "Time Average: 0.006294718027114868\n",
      "Mean Reward: 235.68\n",
      "Episode: 56000\n",
      "Epsilon: 0.10025307881289336\n",
      "Time Average: 0.016445406198501586\n",
      "Mean Reward: 240.113\n",
      "Time Average: 0.006255903482437134\n",
      "Mean Reward: 231.916\n",
      "Episode: 58000\n",
      "Epsilon: 0.09071251019409635\n",
      "Time Average: 0.012705301761627198\n",
      "Mean Reward: 205.485\n",
      "Epsilon: 0.08847275503994115\n",
      "Epsilon: 0.08628830100290674\n",
      "Time Average: 0.0067301905155181885\n",
      "Mean Reward: 251.422\n",
      "Episode: 60000\n",
      "Epsilon: 0.08207986830082019\n",
      "Time Average: 0.014491446256637574\n",
      "Mean Reward: 246.617\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995\n",
    "\n",
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(np.int))\n",
    "\n",
    "\n",
    "for episode in range(EPISODES + 1):\n",
    "    t0 = time.time() # set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) # get the discrete start for the restarted environment\n",
    "    done = False\n",
    "    episode_reward = 0 # reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0:\n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) # take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) # do a random ation\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) # step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward # add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0:\n",
    "            #render\n",
    "            env.render()\n",
    "\n",
    "        if not done:\n",
    "            #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + \\\n",
    "                LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05:\n",
    "        #epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() # episode has finished\n",
    "    episode_total = t1 - t0 # episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward # episode total reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        #every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
